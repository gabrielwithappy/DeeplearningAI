{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question and Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Gemini LLM with a VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Provide your Google API key here\")\n",
    "\n",
    "API_KEY = os.environ[\"GOOGLE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  Model(name='models/chat-bison-001',\n",
       "        base_model_id='',\n",
       "        version='001',\n",
       "        display_name='PaLM 2 Chat (Legacy)',\n",
       "        description='A legacy text-only model optimized for chat conversations',\n",
       "        input_token_limit=4096,\n",
       "        output_token_limit=1024,\n",
       "        supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
       "        temperature=0.25,\n",
       "        top_p=0.95,\n",
       "        top_k=40)),\n",
       " (1,\n",
       "  Model(name='models/text-bison-001',\n",
       "        base_model_id='',\n",
       "        version='001',\n",
       "        display_name='PaLM 2 (Legacy)',\n",
       "        description='A legacy model that understands text and generates text as an output',\n",
       "        input_token_limit=8196,\n",
       "        output_token_limit=1024,\n",
       "        supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
       "        temperature=0.7,\n",
       "        top_p=0.95,\n",
       "        top_k=40)),\n",
       " (2,\n",
       "  Model(name='models/embedding-gecko-001',\n",
       "        base_model_id='',\n",
       "        version='001',\n",
       "        display_name='Embedding Gecko',\n",
       "        description='Obtain a distributed representation of a text.',\n",
       "        input_token_limit=1024,\n",
       "        output_token_limit=1,\n",
       "        supported_generation_methods=['embedText', 'countTextTokens'],\n",
       "        temperature=None,\n",
       "        top_p=None,\n",
       "        top_k=None)),\n",
       " (3,\n",
       "  Model(name='models/gemini-pro',\n",
       "        base_model_id='',\n",
       "        version='001',\n",
       "        display_name='Gemini Pro',\n",
       "        description='The best model for scaling across a wide range of tasks',\n",
       "        input_token_limit=30720,\n",
       "        output_token_limit=2048,\n",
       "        supported_generation_methods=['generateContent', 'countTokens'],\n",
       "        temperature=0.9,\n",
       "        top_p=1.0,\n",
       "        top_k=1)),\n",
       " (4,\n",
       "  Model(name='models/gemini-pro-vision',\n",
       "        base_model_id='',\n",
       "        version='001',\n",
       "        display_name='Gemini Pro Vision',\n",
       "        description='The best image understanding model to handle a broad range of applications',\n",
       "        input_token_limit=12288,\n",
       "        output_token_limit=4096,\n",
       "        supported_generation_methods=['generateContent', 'countTokens'],\n",
       "        temperature=0.4,\n",
       "        top_p=1.0,\n",
       "        top_k=32)),\n",
       " (5,\n",
       "  Model(name='models/embedding-001',\n",
       "        base_model_id='',\n",
       "        version='001',\n",
       "        display_name='Embedding 001',\n",
       "        description='Obtain a distributed representation of a text.',\n",
       "        input_token_limit=2048,\n",
       "        output_token_limit=1,\n",
       "        supported_generation_methods=['embedContent', 'countTextTokens'],\n",
       "        temperature=None,\n",
       "        top_p=None,\n",
       "        top_k=None)),\n",
       " (6,\n",
       "  Model(name='models/aqa',\n",
       "        base_model_id='',\n",
       "        version='001',\n",
       "        display_name='Model that performs Attributed Question Answering.',\n",
       "        description=('Model trained to return answers to questions that are grounded in provided '\n",
       "                     'sources, along with estimating answerable probability.'),\n",
       "        input_token_limit=7168,\n",
       "        output_token_limit=1024,\n",
       "        supported_generation_methods=['generateAnswer'],\n",
       "        temperature=0.2,\n",
       "        top_p=1.0,\n",
       "        top_k=40))]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install google-generativeai\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=API_KEY)\n",
    "genai_models = [(idx, model) for idx,  model in enumerate(genai.list_models())]\n",
    "genai_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install langchain-google-genai\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "gemini_embedding = GoogleGenerativeAIEmbeddings(model='models/embedding-001')\n",
    "persist_directory  = 'docs/chroma'\n",
    "vectordb = Chroma(persist_directory=persist_directory,\n",
    "                  embedding_function = gemini_embedding)\n",
    "vectordb._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Document(page_content='[End of Audio]  \\nDuration: 69 minutes', metadata={'page': 21, 'source': 'pdf/MachineLearning-Lecture01.pdf'}), 0.7836388792148101), (Document(page_content='[End of Audio]  \\nDuration: 69 minutes', metadata={'page': 21, 'source': 'pdf/MachineLearning-Lecture01.pdf'}), 0.7836388792148101), (Document(page_content=\"slowly yourself, go to the course homepage and download detailed lecture notes that \\npretty much describe all the mathematical, te chnical contents I'm going to go over today.  \\nToday, I'm also going to delve into a fair amount  – some amount of linear algebra, and so \\nif you would like to see a refres her on linear algebra, this w eek's discussion section will \\nbe taught by the TAs and will be a refresher on linear algebra. So if some of the linear \\nalgebra I talk about today sort of seems to be going by pretty quickl y, or if you just want \\nto see some of the things I'm claiming today with our proof, if you wa nt to just see some \\nof those things written out in  detail, you can come to this week's discussion section.\", metadata={'page': 0, 'source': 'pdf/MachineLearning-Lecture02.pdf'}), 0.6502406265628284), (Document(page_content=\"times X transpose Y. And so this gives us a wa y for solving for the least square fit to the \\nparameters in closed form, without need ing to use an [inaudible] descent.  \\nOkay, and using this matrix vector notati on, I think, I don't know, I think we did this \\nwhole thing in about ten minut es, which we couldn't have if I was writing out reams of\", metadata={'page': 16, 'source': 'pdf/MachineLearning-Lecture02.pdf'}), 0.6303836025022543), (Document(page_content='entire training set again. So you’re actually  right. If you have a very large training set \\nthen this is a somewhat expensive algorith m to use. Because every time you want to \\nmake a prediction you need to fit a straight li ne to a huge data set again. Turns out there \\nare algorithms that – turns out there are ways to make this much more efficient for large', metadata={'page': 5, 'source': 'pdf/MachineLearning-Lecture03.pdf'}), 0.6237891379968459), (Document(page_content=\"middle of class, but because there won't be video you can safely sit there and make faces \\nat me, and that won't show, okay?  \\nLet's see. I also handed out this — ther e were two handouts I hope most of you have, \\ncourse information handout. So let me just sa y a few words about parts of these. On the \\nthird page, there's a section that says Online Resources.  \\nOh, okay. Louder? Actually, could you turn up the volume? Testing. Is this better? \\nTesting, testing. Okay, cool. Thanks.\", metadata={'page': 4, 'source': 'pdf/MachineLearning-Lecture01.pdf'}), 0.6176415986730468), (Document(page_content=\"middle of class, but because there won't be video you can safely sit there and make faces \\nat me, and that won't show, okay?  \\nLet's see. I also handed out this — ther e were two handouts I hope most of you have, \\ncourse information handout. So let me just sa y a few words about parts of these. On the \\nthird page, there's a section that says Online Resources.  \\nOh, okay. Louder? Actually, could you turn up the volume? Testing. Is this better? \\nTesting, testing. Okay, cool. Thanks.\", metadata={'page': 4, 'source': 'pdf/MachineLearning-Lecture01.pdf'}), 0.6176415986730468)]\n"
     ]
    }
   ],
   "source": [
    "question = \"?\"\n",
    "docs = vectordb.similarity_search_with_relevance_scores(question, k=7)\n",
    "print(docs)\n",
    "# for doc in docs:    \n",
    "#     print(doc.page_content)\n",
    "#     print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", \n",
    "                             convert_system_message_to_human=True,\n",
    "                             temperature=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RetrievalQA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever= vectordb.as_retriever(),\n",
    "    chain_type=\"refine\"    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are goal of this lecture?\"\n",
    "result = qa_chain({'query':question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The new context does not provide any additional information about the goal of the lecture, so I cannot refine the original answer.\\n\\nThe original answer is:\\n\\n> The goal of the lecture is to introduce students to some mathematical and technical content, as well as some linear algebra. The lecturer also mentions that there will be a discussion section this week taught by the TAs that will serve as a refresher on linear algebra. Students who find the lecturer's discussion of linear algebra to be too quick or who want to see more detailed proofs of the claims made in the lecture are encouraged to attend the discussion section.\\n\\nThis answer is still accurate and comprehensive, even with the new context. The new context simply provides some additional information about the logistics of the lecture, such as the availability of online resources and the volume of the lecturer's voice. However, this information does not change the overall goal of the lecture.\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.  Use three sentences maximum. Keep the answer as concise as possiable. Alwasys say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 50, 'fetch_k': 50}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is the class name?\"\n",
    "result = qa_chain.invoke({'query':question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The class name is CS229. Thanks for asking!'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"of this class will not be very program ming intensive, although we will do some \\nprogramming, mostly in either MATLAB or Octa ve. I'll say a bit more about that later.  \\nI also assume familiarity with basic proba bility and statistics. So most undergraduate \\nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \\nassume all of you know what ra ndom variables are, that all of you know what expectation \\nis, what a variance or a random variable is. And in case of some of you, it's been a while \\nsince you've seen some of this material. At some of the discussion sections, we'll actually \\ngo over some of the prerequisites, sort of as  a refresher course under prerequisite class. \\nI'll say a bit more about that later as well.  \\nLastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate \\nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \\n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \\ngonna assume that all of you know what matrix es and vectors are, that you know how to \\nmultiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that'd be even better. \\nBut if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \\nthe review sections.\""
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['source_documents'][19].page_content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
