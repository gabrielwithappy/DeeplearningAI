{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question and Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Gemini LLM with a VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Provide your Google API key here\")\n",
    "\n",
    "API_KEY = os.environ[\"GOOGLE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  Model(name='models/chat-bison-001',\n",
       "        base_model_id='',\n",
       "        version='001',\n",
       "        display_name='PaLM 2 Chat (Legacy)',\n",
       "        description='A legacy text-only model optimized for chat conversations',\n",
       "        input_token_limit=4096,\n",
       "        output_token_limit=1024,\n",
       "        supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
       "        temperature=0.25,\n",
       "        top_p=0.95,\n",
       "        top_k=40)),\n",
       " (1,\n",
       "  Model(name='models/text-bison-001',\n",
       "        base_model_id='',\n",
       "        version='001',\n",
       "        display_name='PaLM 2 (Legacy)',\n",
       "        description='A legacy model that understands text and generates text as an output',\n",
       "        input_token_limit=8196,\n",
       "        output_token_limit=1024,\n",
       "        supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
       "        temperature=0.7,\n",
       "        top_p=0.95,\n",
       "        top_k=40)),\n",
       " (2,\n",
       "  Model(name='models/embedding-gecko-001',\n",
       "        base_model_id='',\n",
       "        version='001',\n",
       "        display_name='Embedding Gecko',\n",
       "        description='Obtain a distributed representation of a text.',\n",
       "        input_token_limit=1024,\n",
       "        output_token_limit=1,\n",
       "        supported_generation_methods=['embedText', 'countTextTokens'],\n",
       "        temperature=None,\n",
       "        top_p=None,\n",
       "        top_k=None)),\n",
       " (3,\n",
       "  Model(name='models/gemini-pro',\n",
       "        base_model_id='',\n",
       "        version='001',\n",
       "        display_name='Gemini Pro',\n",
       "        description='The best model for scaling across a wide range of tasks',\n",
       "        input_token_limit=30720,\n",
       "        output_token_limit=2048,\n",
       "        supported_generation_methods=['generateContent', 'countTokens'],\n",
       "        temperature=0.9,\n",
       "        top_p=1.0,\n",
       "        top_k=1)),\n",
       " (4,\n",
       "  Model(name='models/gemini-pro-vision',\n",
       "        base_model_id='',\n",
       "        version='001',\n",
       "        display_name='Gemini Pro Vision',\n",
       "        description='The best image understanding model to handle a broad range of applications',\n",
       "        input_token_limit=12288,\n",
       "        output_token_limit=4096,\n",
       "        supported_generation_methods=['generateContent', 'countTokens'],\n",
       "        temperature=0.4,\n",
       "        top_p=1.0,\n",
       "        top_k=32)),\n",
       " (5,\n",
       "  Model(name='models/embedding-001',\n",
       "        base_model_id='',\n",
       "        version='001',\n",
       "        display_name='Embedding 001',\n",
       "        description='Obtain a distributed representation of a text.',\n",
       "        input_token_limit=2048,\n",
       "        output_token_limit=1,\n",
       "        supported_generation_methods=['embedContent', 'countTextTokens'],\n",
       "        temperature=None,\n",
       "        top_p=None,\n",
       "        top_k=None)),\n",
       " (6,\n",
       "  Model(name='models/aqa',\n",
       "        base_model_id='',\n",
       "        version='001',\n",
       "        display_name='Model that performs Attributed Question Answering.',\n",
       "        description=('Model trained to return answers to questions that are grounded in provided '\n",
       "                     'sources, along with estimating answerable probability.'),\n",
       "        input_token_limit=7168,\n",
       "        output_token_limit=1024,\n",
       "        supported_generation_methods=['generateAnswer'],\n",
       "        temperature=0.2,\n",
       "        top_p=1.0,\n",
       "        top_k=40))]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=API_KEY)\n",
    "genai_models = [(idx, model) for idx,  model in enumerate(genai.list_models())]\n",
    "genai_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "gemini_embedding = GoogleGenerativeAIEmbeddings(model='models/embedding-001')\n",
    "persist_directory  = 'docs/chroma'\n",
    "vectordb = Chroma(persist_directory=persist_directory,\n",
    "                  embedding_function = gemini_embedding)\n",
    "vectordb._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Document(page_content='[End of Audio]  \\nDuration: 69 minutes', metadata={'page': 21, 'source': 'pdf/MachineLearning-Lecture01.pdf'}), 0.7836388792148101), (Document(page_content='[End of Audio]  \\nDuration: 69 minutes', metadata={'page': 21, 'source': 'pdf/MachineLearning-Lecture01.pdf'}), 0.7836388792148101), (Document(page_content=\"algebra I talk about today sort of seems to be going by pretty quickl y, or if you just want \\nto see some of the things I'm claiming today with our proof, if you wa nt to just see some \\nof those things written out in  detail, you can come to this week's discussion section.\", metadata={'page': 0, 'source': 'pdf/MachineLearning-Lecture02.pdf'}), 0.7207920064528515), (Document(page_content=\"All right, I realize that was a fair amount of  notation, and as I proceed through the rest of \\nthe lecture today, or in future weeks as well,  if some day you're looking at me write a \\nsymbol and you're wondering, gee, what was th at simple lower case N again? Or what \\nwas that lower case X again, or whatever, pleas e raise hand and I'll answer. This is a fair\", metadata={'page': 3, 'source': 'pdf/MachineLearning-Lecture02.pdf'}), 0.6553166008941993), (Document(page_content='And so it turns out that this sort of clustering algorithm or this sort of unsupervised \\nlearning algorithm, which learns  to group pixels together, it turns out to be useful for \\nmany applications in vision, in co mputer vision image processing.', metadata={'page': 15, 'source': 'pdf/MachineLearning-Lecture01.pdf'}), 0.6551109056425048), (Document(page_content='And so it turns out that this sort of clustering algorithm or this sort of unsupervised \\nlearning algorithm, which learns  to group pixels together, it turns out to be useful for \\nmany applications in vision, in co mputer vision image processing.', metadata={'page': 15, 'source': 'pdf/MachineLearning-Lecture01.pdf'}), 0.6551109056425048), (Document(page_content='out here, it cosmetically looks a bit like a Ga ussian distribution. Okay? But this actually \\nhas absolutely nothing to  do with Gaussian distribution. So this is not that a problem with \\nXI is Gaussian or whatever. This is no su ch interpretation. This is just a convenient \\nfunction that happens to be a bell-shaped f unction, but don’t endow this of any Gaussian \\nsemantics. Okay?', metadata={'page': 3, 'source': 'pdf/MachineLearning-Lecture03.pdf'}), 0.6150175115840435)]\n"
     ]
    }
   ],
   "source": [
    "question = \"?\"\n",
    "docs = vectordb.similarity_search_with_relevance_scores(question, k=7)\n",
    "print(docs)\n",
    "# for doc in docs:    \n",
    "#     print(doc.page_content)\n",
    "#     print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", \n",
    "                             convert_system_message_to_human=True)\n",
    "                            #  temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RetrievalQA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever= vectordb.as_retriever(),\n",
    "    chain_type=\"refine\"    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are goal of this lecture?\"\n",
    "result = qa_chain({'query':question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The new context does not provide any additional information about the goals of the lecture, so the original answer is still the best response to the question.\\n\\nThe duration of the lecture (69 minutes) is not relevant to the goals of the lecture.\\n\\nTherefore, the original answer is still the best response to the question:\\n\\n**The goal of this lecture, and indeed the entire course, is to teach students how to use machine learning tools effectively and responsibly.**\\n\\nThe lecturer believes that it is important to teach students how to use machine learning tools effectively because these tools are becoming increasingly powerful and are being used in a wide variety of applications. It is important that students are able to use these tools responsibly and ethically.\\n\\nIn this lecture, the lecturer will provide students with some general principles for using machine learning tools well. These principles include:\\n\\n* **Start with a clear problem definition.** What do you want your machine learning model to achieve?\\n* **Choose the right data.** The data you use to train your model is critical to its success.\\n* **Use the right algorithm.** There are many different machine learning algorithms available, and the best one for your task will depend on the specific problem you are trying to solve.\\n* **Tune your model's hyperparameters.** Hyperparameters are settings that control the behavior of a machine learning algorithm. Tuning these hyperparameters can help to improve the performance of your model.\\n* **Evaluate your model carefully.** How well does your model perform on new data? Is it robust to noise and outliers?\\n* **Deploy your model responsibly.** Once you have a machine learning model that you are confident in, you need to deploy it in a responsible manner. This includes considering the potential ethical implications of your model.\\n\\nBy following these principles, students can learn to use machine learning tools effectively and responsibly.\""
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.  Use three sentences maximum. Keep the answer as concise as possiable. Alwasys say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 10, 'fetch_k': 50}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is the name of the class?\"\n",
    "result = qa_chain.invoke({'query':question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I cannot find the answer to what the name of the class is from the provided context. Thanks for asking!\""
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/gabrielyang/00_PRJ/dev_group/langchain_study/Question_Answering.ipynb 셀 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gabrielyang/00_PRJ/dev_group/langchain_study/Question_Answering.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m result[\u001b[39m'\u001b[39;49m\u001b[39msource_documents\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m5\u001b[39;49m]\u001b[39m.\u001b[39mpage_content\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "result['source_documents'][5].page_content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
